Describir brevemente un panorama general del presente informe. 
(1 cuartilla)


DESARROLLO DE ACTIVIDADES Y RESULTADOS OBTENIDOS



Se mencionarán detalladamente las actividades realizadas por el prestador (a) de servicio social en la Institución receptora y/o comunidad, así como los resultados obtenidos en cada una.  
(De 3 a 6 cuartillas)


Análisis de datos de parámetros atmosféricos y ambientales medidos en el Observatorio Atmosférico Calakmul. 

Primera revisión de las bases de datos generadas por los instrumentos instalados en el observatorio, Organización y actualización de los repositorios, discusión de las estrategias para el manejo de datos.


Se descargó todo el software para llevar a cabo la revisión de datos fue Google Colab, Visual Code Estudio, Python 3 y la plataforma Github. 

Los datos utilizados para revisión de datos del Observatorio Atmosferico de Calakmul, se tomaron de los servidores de la ROUA, los cuales ya cuentan con datos de tipo l1

Se tomaron los datos de toda la serie de tiempo disponible desde noviembre de 2023 hasta Marzo de 2025. 

Los datos contaban con inconsistencias y perdida de datos producto de fallas en la alimentación u fenómenos ajenos que hicieron que los sensores no regustraran datos. 



Se corrigieron los datos que tenían un comportamiento anómalo en la serie de tiempo de Temperatura. Se realizó una regresión lineal basada en las observaciones de otro sensor, el PMT640 Mass Monitor, que también cuenta con sensor de temperatura. Los datos se utilizaron para correlacionar valores faltantes en la serie de meteorología usando registros existentes del PMT640. Además, se agregaron flags a las columnas para indicar dónde se aplicó la regresión lineal ('RL') o la interpolación ('IP').

Finalmente se entrego un archivo de tipo L2 que contenía estas correcciones en tipo .CSV





2. Elaboración de Scripts para el filtrado, control de calidad y visualización de los datos de distintos sensores.  

Este apartado se detallarán los diferentes métodos y algoritmos utilizados para el procesamiento de los distintos sensores. 
La codificación de scripts para leer la información capturada en los archivos con datos crudos. Definición de criterios de filtrado y depuración de datos incorrectos para la elaboración de nivel 1 y nivel 2 de las bases de datos con controles de calidad. Creación de promedios y tablas estadísticas de los datos. Elaboración de scripts para la visualización de la información en formato de series de tiempo, histogramas, rosas de viento y de contaminantes, correlaciones, etc.


PMT640 Mass Monitor.

Es un instrumento para la lectura en tiempo real de material particulado PM10 y PM2.5.

Se trabajaron con los datos crudos de toda la serie de tiempo.

Como primer paso, se revisó que la serie de tiempo estuviera completa, se agregaron fechas faltantes y se renombraron las columnas a PM10 y PM2.5. Se creó un nuevo repositorio donde se guardaron todos los archivos en formato minutal y por hora, con un nuevo formato de nombre: datos_año.ccv

Se llevan a cabo el ploteo de varias figuras combinando los archivos de meteorología con los de partículas y se llevan a cabo rosas de viento los cuales para llevar a cabo el promedio se tiene que calcular con la fórmula tal juntando meteorología y partículas se obtuvieron las siguientes figuras que para análisis futuros pueden ayudar a comprender mejor eventos donde mucho material particulado provenga de algún sitio en particular




Picarro Gas Concetration Analyzers

Analiza simultáneamente monóxido de carbono (CO), dióxido de carbono (CO2) y metano(CH4), gases de efecto invernadero GEI de interes publico y climatico. 
La ROUA cuenta con scripts para el tratamiento de datos de Picarro, pero para algunos datos de Calakmul el procesamiento no funcionaba, asi que basandose en la forma de limpiar datos del script y sugerencias revisadas con el asesor, se decidio trabajar con los datos crudos, crear un script propio para procesar toda la serie de tiempo de Calakmul. 
Se trabajaron con los datos crudos para toda la serie de tiempo, a trabajar con los datos crudos fue necesario buscar otro tipo de separador de datos para hacer el procesamiento y debido a la cantidad y tamaño de los datos se hizo en varias partes una parte del procesamiento fue directamente realizada dentro del servidor 32 del instituto de ciencias atmosféricas, fue necesario filtrar a través de the flags los valores que correspondían la posición de la válvula teniendo ese cuatro valores para cada gas se ocupó una combinación  aquí se inserta una tabla con los flags y la columna 
dado que el analizador da en una frecuencia dede picarro sin embargo algunos problemas en el procesamiento
El primer paso fue obtener los datos minutatales una vez ya filtrados con los flash de posición de válvula y obtener la desviación estándar 
Segundo paso fue corregir la línea de tiempo utc -6h y una corrección de 170 segundos que se tienen desde la válvula en la cima de la estación hasta el analizador de gases dentro de el contenedor.
 A la serie de tiempo se le aplicó un umbral basado en los valores promedio de la desviación estándar obtenida previamente valores que no correspondieron a valores conocidos de concentración de los gases fueron eliminados 
Debido a la pérdida de datos monóxido de carbono ya no se trabajaron esos datos pero sí con datos de co2 y ch4 los cuales se quitaron los valores menores a $300 ppm para co2 y menores a 1.6 ppm para ch4 ya que estos ya son concentraciones menores a las promedio mundial. 
 
/Se llevaron a cabo las series de tiempo anual para el año 2024 y se empezó a trabajar con perfiles de tiempo esto tomando como base en promedio por hora por cada mes para así obtener un perfil de cómo se comportan los gases anualmente 
Que administraron los promedios horarios con distintos intervalos de tiempo en eso tuvieron promedios de las 19 horas en horas de las 9 a las 16 de todo el día así como su promedio anual. 


Se discutieron los datos y se llegó a la conclusión de que se debe llevar a cabo otro umbral para eliminar más valores por desviación estándar para producir un nuevo producto l1b y así limpiar un poco de ruido en la serie de tiempo se borró datos de desviación estándar mayor o igual a 0.2 para co2 y desviación estándar mayor igual eh 0.002 para cch4 
Se compararon ambas series de tiempo l1 y l1 b y se decidió dejar el formato de l1b que muestra los valores más suavizados  


Pandoras 

Pandora es una red global lanzada por la NASA en colaboración con la esa la estación espacial europea que establecen estaciones de medición de observaciones de concentración de casa estraza en columnas totales y verticales para apoyar a la validación y verificación de sensores v visible en satélites órbitas bajas y gestacionales.
 
Datos de nivel L2 se trabajó con un script previo pero se creó un nuevo script a partir de Data frames para modernizar el código y hacerlo más replicable ahora bien se revisó el número de columnas de interés la columna de tiempo columna de no2 y flags 
El valor de tiempo tiene un formato ISO 86 aquí se muestra el formato de por lo que es necesario pasar al formato año mes día hora minuto segundo 
Y aplicarle la corrección tu tc menos 6 horas para cada valor se decidió tomar los valores debido a que la corrección L2 deja pocos datos en un intervalo de aproximadamente las 7 a.m a las 17 p.m 
Similarmente a lo trabajado con los datos gei de picarro se trabajó con valores anuales en intervalo de tiempo de 24 horas para cada mes dividido en su blogs por mes debido a que la serie de tiempo de Pandora de calakmul no está completa no es posible llevar a cabo toda la serie para todo el año 2024 esta se realizó con solo los datos obtenidos.



Apoyo en la instalación y revisión de instrumentos atmosféricos en
campo.

Revisión de sensores para la medición deparámetros micrometeorológicos y de respiración de suelo. Configuración y adecuación de equipos para la adquisición de los datos en formatos requeridos. Pruebas en laboratorio. Instalación y operación de equipos en campo




EXPERIENCIA EN TÉRMINOS DE LO SOCIAL, FORMATIVO Y RETRIBUTIVO
Durante mi estancia en el grupo de espectroscopia me permitió conocer de manera más directa cómo se mueve un grupo de trabajo aprendiendo a procesar datos al utilizar herramientas como programación software, pude colaborar y estar presente durante reuniones donde se discutía fases de proyectos y tópicos que lleva el grupo así mismo el acercamiento que tuve con mi asesor el doctor Michael kruter me permitió conocer mucho mejor cómo se puede trabajar estos datos y durante todo el servicio social se discutía acerca de qué datos iban obteniendo y cómo se podrían mejorar cosas o implementar nuevas formas de algoritmos ayudar a la interpretación, esto sin duda es clave pues si bien puedes aprender a manejar los datos estos solo pueden ser datos si no sabes darles una interpretación o simplemente no sabes para qué utilizarlos así que considere que el hecho de aprender a discriminar errores identificar eventos y conocer de qué manera tienen que verse los datos que se quieran analizar me permite discriminar mejor qué tipo de dato quiero obtener y para qué lo quiero utilizar.
Haber participado en ese grupo le dio más sentido a los años dedicados durante la carrera puesto que todos mis conocimientos aplicados durante ese trabajo y sentí mucha satisfacción el poder ser de utilidad a un grupo ayudándoles a procesar datos obtener nuevos productos y también participar para colaborar en nuevas ideas y mejoras, me sentía arropado por todo el grupo de espectroscopia pues todos ellos se han portado de manera muy cálida y me han ofrecido su apoyo para cualquier cosa que necesite así mismo considero que la interacción con el doctor Michel gruter fue muy buena pues al mantener contacto y comunicación constante podemos resolver dudas y también estar al tanto de las actividades y considero que el hecho de estar en un un grupo donde pueda sentirte cómodo y tengas un ambiente favorable ayuda a que puedas realizar tus actividades de la mejor manera.
 Así mismo la curva de aprendizaje que tuve fue fue incrementandoy considero que las actitudes y todo el conocimiento que obtuve durante la realización de este servicio social me van a ayudar bastante e en el ámbito profesional y académico así como ayudarme a envolverme y perder el miedo de pertenecer a un grupo donde crees que tu falta de experiencia o que apenas ser muy reciente tu formación no puedas aportar nada esto es una total mentira pues durante el servicio social hacemos lo que puede ser capaz de colaborar y ayudar a realizar las actividades descritas así mismo me llevé muchísimo conocimiento que académicamente y para el trabajo de titulación me van a ayudar demasisiado





Se destacará las acciones realizadas más importantes en la entidad receptora donde se prestó el servicio social.
(1 a 2 cuartillas)


La creación de scripts para el procesamiento de datos pues permitió involucrarme en técnicas de análisis de datos utilizando librerías en lenguaje de programación python que permiten trabajar con base de datos con series de tiempo de varios miles de mediciones y que no era posible realizar en hojas de cálculo como Excel pues el procesamiento tardaría mucho más tiempo y sería muy difícil trabajar con tal cantidad de datos por lo que aprender a utilizar los conocimientos adquiridos durante la carrera sobre el procesamiento de datos permitió primero extraer los datos de cada sensor a un formato más trabajable segundo permitió revisar y hacer series de tiempo para observar qué tipo de dato obteníamos es decir una primera aproximación a los datos en crudo para verificar que los datos trabajados tienen coherencia. Y posteriormente empezar a trabajar con correcciones que permitan dar productos de cierto nivel de confianza como l1 L2 que contienen revisiones en los datos así mismo los datos trabajados se extrajeron en archivos de texto u hojas de cálculo punto csv que permiten trabajar con los datos en diferentes plataformas además de homogeneizar la información lo que permite trabajar entre distintos productos ejemplo al trabajar y obtener productos de meteorología y estar homogeneizados en en fecha se puede trabajar también con productos por ejemplo de gases o material particulado obteniendo así un mejor análisis para productos posteriores donde se comparaba la meteorología como dirección y velocidad de viento con la cantidad de material particulado es por ello que una de las actividades más importantes fue homogenizar los datos trabajarlos en un formato más sencillo y replicable y dejar los scripts de manera replicable para que puedan seguir trabajando en el procesamiento de datos lo cual es una tarea sumamente importante que permite que los datos puedan seguir trabajándose y se puedan replicar para quienes quieran utilizar los datos tanto como los productos ya terminados o revisar o llevar a cabo sus procesamientos desde los datos crudos


Material que refuerce la experiencia y evidencia del(a) prestador de servicio social como fotografías, reportes, links de vídeos propios, etc.
(1 a 3 cuartillas)

